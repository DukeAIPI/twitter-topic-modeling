{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3233fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from searchtweets import load_credentials, gen_request_parameters, collect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7782486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(term):\n",
    "    credentials = load_credentials(env_overwrite=True)\n",
    "    query = gen_request_parameters(f\"{term} -is:retweet -is:reply lang:en\", results_per_call=100, granularity=None)\n",
    "    tweets = collect_results(query, max_tweets = 1000, result_stream_args=credentials)\n",
    "    return tweets\n",
    "\n",
    "def convert_to_df(messy_tweets):\n",
    "    '''Turns messy json repsonse from get_tweets into a clean df.\n",
    "    \n",
    "    Args:\n",
    "        messy_tweets (list of dicts extracted from json response): 'data' part of .json() parse\n",
    "        \n",
    "    Returns:\n",
    "        df: df that has the extracted tweets\n",
    "    '''\n",
    "    clean = []\n",
    "    \n",
    "    for i in range(len(messy_tweets)):\n",
    "        for j in range(len(messy_tweets[i]['data'])):\n",
    "            clean.append(messy_tweets[i]['data'][j]['text'])\n",
    "    \n",
    "    df = pd.DataFrame(clean, columns = ['tweet'])\n",
    "    df = df.drop_duplicates(ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c870d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot read file ~/.twitter_keys.yaml\n",
      "Error parsing YAML file; searching for valid environment variables\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excited to get our first episode of \"Trips aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>⚡️Shmyhal: Ukraine's GDP may drop by 30-50% du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#latestnews Russian economy crumbling as offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$VRT - Vertiv Holdings' (VRT) CEO Rob Johnson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://t.co/z7hHiOu6jQ  In 2019, Latinos adde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>Countries in #SouthAsia should steer away from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>#Airfare Deal: [American] New York - Dallas (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>Naturally we are aware of the strength of our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>But it's good that someone of international si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>Watch: House Republican-Aligned Campaign Arm R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet\n",
       "0    Excited to get our first episode of \"Trips aro...\n",
       "1    ⚡️Shmyhal: Ukraine's GDP may drop by 30-50% du...\n",
       "2    #latestnews Russian economy crumbling as offic...\n",
       "3    $VRT - Vertiv Holdings' (VRT) CEO Rob Johnson ...\n",
       "4    https://t.co/z7hHiOu6jQ  In 2019, Latinos adde...\n",
       "..                                                 ...\n",
       "983  Countries in #SouthAsia should steer away from...\n",
       "984  #Airfare Deal: [American] New York - Dallas (a...\n",
       "985  Naturally we are aware of the strength of our ...\n",
       "986  But it's good that someone of international si...\n",
       "987  Watch: House Republican-Aligned Campaign Arm R...\n",
       "\n",
       "[988 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messy_tweets = get_tweets(\"Economy\")\n",
    "tweets = convert_to_df(messy_tweets)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab33a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5966fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "docs_clean = [clean(doc).split() for doc in tweets[\"tweet\"]] # tokenized, put everything lowercase, removed stop words, removed punctuation, lemmatized everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93474ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(docs_clean)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b95ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                         num_topics=10, \n",
    "                                         id2word=dictionary, \n",
    "                                         passes=4, \n",
    "                                         workers=4,\n",
    "                                         random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce9ad567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.037*\"economy\" + 0.004*\"could\" + 0.004*\"never\" + 0.004*\"growth\" + 0.004*\"good\" + 0.003*\"get\" + 0.003*\"billion\" + 0.003*\"go\" + 0.003*\"money\" + 0.003*\"russia\"\n",
      "Topic: 1 \n",
      "Words: 0.027*\"economy\" + 0.006*\"business\" + 0.006*\"amp\" + 0.005*\"gas\" + 0.004*\"tax\" + 0.004*\"california\" + 0.004*\"make\" + 0.004*\"people\" + 0.004*\"nation\" + 0.004*\"already\"\n",
      "Topic: 2 \n",
      "Words: 0.040*\"economy\" + 0.008*\"gas\" + 0.005*\"u\" + 0.005*\"business\" + 0.004*\"amp\" + 0.004*\"tax\" + 0.004*\"would\" + 0.004*\"russia\" + 0.004*\"russian\" + 0.004*\"finance\"\n",
      "Topic: 3 \n",
      "Words: 0.017*\"economy\" + 0.010*\"amp\" + 0.005*\"inflation\" + 0.004*\"economic\" + 0.003*\"price\" + 0.003*\"need\" + 0.003*\"fact\" + 0.003*\"energy\" + 0.003*\"new\" + 0.003*\"pm\"\n",
      "Topic: 4 \n",
      "Words: 0.032*\"economy\" + 0.006*\"among\" + 0.006*\"picture\" + 0.006*\"loss\" + 0.006*\"pressure\" + 0.006*\"money\" + 0.006*\"land\" + 0.006*\"test\" + 0.006*\"benefit\" + 0.006*\"debate\"\n",
      "Topic: 5 \n",
      "Words: 0.049*\"economy\" + 0.006*\"business\" + 0.006*\"—\" + 0.006*\"want\" + 0.005*\"via\" + 0.005*\"u\" + 0.004*\"inflation\" + 0.004*\"people\" + 0.004*\"fed\" + 0.003*\"help\"\n",
      "Topic: 6 \n",
      "Words: 0.025*\"economy\" + 0.005*\"bitcoin\" + 0.005*\"republic\" + 0.005*\"african\" + 0.005*\"central\" + 0.004*\"use\" + 0.004*\"inflation\" + 0.004*\"used\" + 0.004*\"btc\" + 0.003*\"new\"\n",
      "Topic: 7 \n",
      "Words: 0.028*\"economy\" + 0.004*\"people\" + 0.004*\"war\" + 0.003*\"ukraine\" + 0.003*\"economic\" + 0.003*\"world\" + 0.003*\"need\" + 0.003*\"post\" + 0.003*\"energy\" + 0.003*\"russia\"\n",
      "Topic: 8 \n",
      "Words: 0.038*\"economy\" + 0.007*\"u\" + 0.006*\"tax\" + 0.006*\"people\" + 0.005*\"make\" + 0.005*\"nation\" + 0.005*\"country\" + 0.005*\"gas\" + 0.004*\"california\" + 0.004*\"spur\"\n",
      "Topic: 9 \n",
      "Words: 0.045*\"economy\" + 0.020*\"u\" + 0.013*\"pay\" + 0.012*\"future\" + 0.011*\"web3\" + 0.011*\"now\" + 0.011*\"koiinetwork\" + 0.011*\"joined\" + 0.011*\"koii\" + 0.011*\"attention\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cc077ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=tweets[\"tweet\"]):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in tqdm(enumerate(ldamodel[corpus])):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a47ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "988it [00:01, 858.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>economy, among, picture, loss, pressure, money...</td>\n",
       "      <td>Excited to get our first episode of \"Trips aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>economy, business, —, want, via, u, inflation,...</td>\n",
       "      <td>⚡️Shmyhal: Ukraine's GDP may drop by 30-50% du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>economy, amp, inflation, economic, price, need...</td>\n",
       "      <td>#latestnews Russian economy crumbling as offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>economy, gas, u, business, amp, tax, would, ru...</td>\n",
       "      <td>$VRT - Vertiv Holdings' (VRT) CEO Rob Johnson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>economy, business, —, want, via, u, inflation,...</td>\n",
       "      <td>https://t.co/z7hHiOu6jQ  In 2019, Latinos adde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>economy, amp, inflation, economic, price, need...</td>\n",
       "      <td>Countries in #SouthAsia should steer away from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.7136</td>\n",
       "      <td>economy, u, tax, people, make, nation, country...</td>\n",
       "      <td>#Airfare Deal: [American] New York - Dallas (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>economy, people, war, ukraine, economic, world...</td>\n",
       "      <td>Naturally we are aware of the strength of our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>economy, could, never, growth, good, get, bill...</td>\n",
       "      <td>But it's good that someone of international si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>economy, bitcoin, republic, african, central, ...</td>\n",
       "      <td>Watch: House Republican-Aligned Campaign Arm R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>988 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic  Perc_Contribution  \\\n",
       "0               4.0             0.9735   \n",
       "1               5.0             0.9719   \n",
       "2               3.0             0.9307   \n",
       "3               2.0             0.9500   \n",
       "4               5.0             0.9690   \n",
       "..              ...                ...   \n",
       "983             3.0             0.9667   \n",
       "984             8.0             0.7136   \n",
       "985             7.0             0.9308   \n",
       "986             0.0             0.9571   \n",
       "987             6.0             0.9307   \n",
       "\n",
       "                                        Topic_Keywords  \\\n",
       "0    economy, among, picture, loss, pressure, money...   \n",
       "1    economy, business, —, want, via, u, inflation,...   \n",
       "2    economy, amp, inflation, economic, price, need...   \n",
       "3    economy, gas, u, business, amp, tax, would, ru...   \n",
       "4    economy, business, —, want, via, u, inflation,...   \n",
       "..                                                 ...   \n",
       "983  economy, amp, inflation, economic, price, need...   \n",
       "984  economy, u, tax, people, make, nation, country...   \n",
       "985  economy, people, war, ukraine, economic, world...   \n",
       "986  economy, could, never, growth, good, get, bill...   \n",
       "987  economy, bitcoin, republic, african, central, ...   \n",
       "\n",
       "                                                 tweet  \n",
       "0    Excited to get our first episode of \"Trips aro...  \n",
       "1    ⚡️Shmyhal: Ukraine's GDP may drop by 30-50% du...  \n",
       "2    #latestnews Russian economy crumbling as offic...  \n",
       "3    $VRT - Vertiv Holdings' (VRT) CEO Rob Johnson ...  \n",
       "4    https://t.co/z7hHiOu6jQ  In 2019, Latinos adde...  \n",
       "..                                                 ...  \n",
       "983  Countries in #SouthAsia should steer away from...  \n",
       "984  #Airfare Deal: [American] New York - Dallas (a...  \n",
       "985  Naturally we are aware of the strength of our ...  \n",
       "986  But it's good that someone of international si...  \n",
       "987  Watch: House Republican-Aligned Campaign Arm R...  \n",
       "\n",
       "[988 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=tweets[\"tweet\"])\n",
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8508623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.32312738143081715\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs_clean, dictionary=lda_model.id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03608755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

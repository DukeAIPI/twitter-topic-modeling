{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3233fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from searchtweets import load_credentials, gen_request_parameters, collect_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7782486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(term):\n",
    "    credentials = load_credentials(env_overwrite=True)\n",
    "    query = gen_request_parameters(f\"{term} -is:retweet -is:reply lang:en\", results_per_call=100, granularity=None)\n",
    "    tweets = collect_results(query, max_tweets = 1000, result_stream_args=credentials)\n",
    "    return tweets\n",
    "\n",
    "def convert_to_df(messy_tweets):\n",
    "    '''Turns messy json repsonse from get_tweets into a clean df.\n",
    "    \n",
    "    Args:\n",
    "        messy_tweets (list of dicts extracted from json response): 'data' part of .json() parse\n",
    "        \n",
    "    Returns:\n",
    "        df: df that has the extracted tweets\n",
    "    '''\n",
    "    clean = []\n",
    "    \n",
    "    for i in range(len(messy_tweets)):\n",
    "        for j in range(len(messy_tweets[i]['data'])):\n",
    "            clean.append(messy_tweets[i]['data'][j]['text'])\n",
    "    \n",
    "    df = pd.DataFrame(clean, columns = ['tweet'])\n",
    "    df = df.drop_duplicates(ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c870d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot read file ~/.twitter_keys.yaml\n",
      "Error parsing YAML file; searching for valid environment variables\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One thing that will never die in this economy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just joined the @KoiiNetwork Web3 economy. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The latest The sharing economy Daily! https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your advice is rubbish Cde 4 Fingers, it shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Great Resignation is rapidly turning into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Culture First, Economy Next\\n\\n（I Suman）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Economy Is Rebounding – Ken Ofori-Atta https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Russian Oil, Markets, And The US Economy (Podc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Economy Is Rebounding – Ken Ofori-Atta https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>CONNECTIONIVOIRIENNE:  Taxes in Ivory Coast re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>955 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweet\n",
       "0    One thing that will never die in this economy ...\n",
       "1    I just joined the @KoiiNetwork Web3 economy. P...\n",
       "2    The latest The sharing economy Daily! https://...\n",
       "3    Your advice is rubbish Cde 4 Fingers, it shoul...\n",
       "4    The Great Resignation is rapidly turning into ...\n",
       "..                                                 ...\n",
       "950           Culture First, Economy Next\\n\\n（I Suman）\n",
       "951  Economy Is Rebounding – Ken Ofori-Atta https:/...\n",
       "952  Russian Oil, Markets, And The US Economy (Podc...\n",
       "953  Economy Is Rebounding – Ken Ofori-Atta https:/...\n",
       "954  CONNECTIONIVOIRIENNE:  Taxes in Ivory Coast re...\n",
       "\n",
       "[955 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messy_tweets = get_tweets(\"Economy\")\n",
    "tweets = convert_to_df(messy_tweets)\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab33a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5966fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "docs_clean = [clean(doc).split() for doc in tweets[\"tweet\"]] # tokenized, put everything lowercase, removed stop words, removed punctuation, lemmatized everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93474ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(docs_clean)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13b95ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, \n",
    "                                         num_topics=10, \n",
    "                                         id2word=dictionary, \n",
    "                                         passes=4, \n",
    "                                         workers=4,\n",
    "                                         random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9ad567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.036*\"economy\" + 0.008*\"business\" + 0.007*\"u\" + 0.006*\"make\" + 0.005*\"help\" + 0.005*\"—\" + 0.004*\"professional\" + 0.004*\"administrative\" + 0.004*\"work\" + 0.004*\"want\"\n",
      "Topic: 1 \n",
      "Words: 0.017*\"economy\" + 0.004*\"one\" + 0.004*\"today\" + 0.004*\"vote\" + 0.003*\"get\" + 0.003*\"information\" + 0.003*\"peter\" + 0.003*\"much\" + 0.003*\"want\" + 0.003*\"make\"\n",
      "Topic: 2 \n",
      "Words: 0.033*\"economy\" + 0.005*\"inflation\" + 0.004*\"u\" + 0.004*\"digital\" + 0.004*\"like\" + 0.004*\"great\" + 0.004*\"know\" + 0.004*\"year\" + 0.004*\"news\" + 0.003*\"tax\"\n",
      "Topic: 3 \n",
      "Words: 0.033*\"economy\" + 0.010*\"amp\" + 0.008*\"global\" + 0.005*\"gas\" + 0.005*\"world\" + 0.005*\"hit\" + 0.004*\"one\" + 0.004*\"good\" + 0.004*\"new\" + 0.004*\"russia\"\n",
      "Topic: 4 \n",
      "Words: 0.028*\"economy\" + 0.011*\"u\" + 0.011*\"trade\" + 0.008*\"power\" + 0.008*\"year\" + 0.007*\"amp\" + 0.006*\"deficit\" + 0.006*\"good\" + 0.006*\"billion\" + 0.006*\"jump\"\n",
      "Topic: 5 \n",
      "Words: 0.038*\"economy\" + 0.009*\"u\" + 0.005*\"help\" + 0.003*\"new\" + 0.003*\"business\" + 0.003*\"market\" + 0.003*\"year\" + 0.003*\"2022\" + 0.003*\"minister\" + 0.002*\"building\"\n",
      "Topic: 6 \n",
      "Words: 0.059*\"economy\" + 0.052*\"future\" + 0.052*\"pay\" + 0.051*\"now\" + 0.051*\"web3\" + 0.051*\"attention\" + 0.051*\"u\" + 0.051*\"joined\" + 0.050*\"koii\" + 0.050*\"koiinetwork\"\n",
      "Topic: 7 \n",
      "Words: 0.037*\"economy\" + 0.005*\"year\" + 0.004*\"amp\" + 0.004*\"make\" + 0.004*\"professional\" + 0.004*\"administrative\" + 0.003*\"important\" + 0.003*\"job\" + 0.003*\"million\" + 0.003*\"gas\"\n",
      "Topic: 8 \n",
      "Words: 0.030*\"economy\" + 0.005*\"market\" + 0.005*\"people\" + 0.005*\"student\" + 0.004*\"canada\" + 0.004*\"debt\" + 0.004*\"would\" + 0.004*\"inflation\" + 0.004*\"1\" + 0.004*\"could\"\n",
      "Topic: 9 \n",
      "Words: 0.046*\"economy\" + 0.007*\"year\" + 0.007*\"inflation\" + 0.006*\"people\" + 0.006*\"word\" + 0.005*\"via\" + 0.005*\"recession\" + 0.005*\"war\" + 0.005*\"american\" + 0.004*\"past\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cc077ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=tweets[\"tweet\"]):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in tqdm(enumerate(ldamodel[corpus])):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a47ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "955it [00:01, 852.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Perc_Contribution</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>economy, year, inflation, people, word, via, r...</td>\n",
       "      <td>One thing that will never die in this economy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>economy, future, pay, now, web3, attention, u,...</td>\n",
       "      <td>I just joined the @KoiiNetwork Web3 economy. P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>economy, u, trade, power, year, amp, deficit, ...</td>\n",
       "      <td>The latest The sharing economy Daily! https://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>economy, one, today, vote, get, information, p...</td>\n",
       "      <td>Your advice is rubbish Cde 4 Fingers, it shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>economy, inflation, u, digital, like, great, k...</td>\n",
       "      <td>The Great Resignation is rapidly turning into ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>economy, year, inflation, people, word, via, r...</td>\n",
       "      <td>Culture First, Economy Next\\n\\n（I Suman）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5691</td>\n",
       "      <td>economy, amp, global, gas, world, hit, one, go...</td>\n",
       "      <td>Economy Is Rebounding – Ken Ofori-Atta https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>economy, year, inflation, people, word, via, r...</td>\n",
       "      <td>Russian Oil, Markets, And The US Economy (Podc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>economy, amp, global, gas, world, hit, one, go...</td>\n",
       "      <td>Economy Is Rebounding – Ken Ofori-Atta https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>economy, year, amp, make, professional, admini...</td>\n",
       "      <td>CONNECTIONIVOIRIENNE:  Taxes in Ivory Coast re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>955 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Dominant_Topic  Perc_Contribution  \\\n",
       "0               9.0             0.9250   \n",
       "1               6.0             0.9250   \n",
       "2               4.0             0.9100   \n",
       "3               1.0             0.9719   \n",
       "4               2.0             0.9100   \n",
       "..              ...                ...   \n",
       "950             9.0             0.8714   \n",
       "951             3.0             0.5691   \n",
       "952             9.0             0.7822   \n",
       "953             3.0             0.8714   \n",
       "954             7.0             0.9437   \n",
       "\n",
       "                                        Topic_Keywords  \\\n",
       "0    economy, year, inflation, people, word, via, r...   \n",
       "1    economy, future, pay, now, web3, attention, u,...   \n",
       "2    economy, u, trade, power, year, amp, deficit, ...   \n",
       "3    economy, one, today, vote, get, information, p...   \n",
       "4    economy, inflation, u, digital, like, great, k...   \n",
       "..                                                 ...   \n",
       "950  economy, year, inflation, people, word, via, r...   \n",
       "951  economy, amp, global, gas, world, hit, one, go...   \n",
       "952  economy, year, inflation, people, word, via, r...   \n",
       "953  economy, amp, global, gas, world, hit, one, go...   \n",
       "954  economy, year, amp, make, professional, admini...   \n",
       "\n",
       "                                                 tweet  \n",
       "0    One thing that will never die in this economy ...  \n",
       "1    I just joined the @KoiiNetwork Web3 economy. P...  \n",
       "2    The latest The sharing economy Daily! https://...  \n",
       "3    Your advice is rubbish Cde 4 Fingers, it shoul...  \n",
       "4    The Great Resignation is rapidly turning into ...  \n",
       "..                                                 ...  \n",
       "950           Culture First, Economy Next\\n\\n（I Suman）  \n",
       "951  Economy Is Rebounding – Ken Ofori-Atta https:/...  \n",
       "952  Russian Oil, Markets, And The US Economy (Podc...  \n",
       "953  Economy Is Rebounding – Ken Ofori-Atta https:/...  \n",
       "954  CONNECTIONIVOIRIENNE:  Taxes in Ivory Coast re...  \n",
       "\n",
       "[955 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=bow_corpus, texts=tweets[\"tweet\"])\n",
    "df_topic_sents_keywords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
